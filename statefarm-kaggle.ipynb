{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StateFarm Kaggle Challenge\n",
    "\n",
    "- For this Kaggle Challenge, I will be performing a rigorous analysis of the dataset provided by StateFarm to predict whether or not the image of a driver is in a distracted or non distracted phase.\n",
    "\n",
    "\n",
    "- StateFarm has provided labelled training data in the form of images of drivers that have been classified in one of 10 different states.\n",
    "\n",
    "\n",
    "- The states of the drivers are :\n",
    "    - c0 : Safe Driving (2489 images)\n",
    "    - c1 : texting - right (2267 images)\n",
    "    - c2 : talking on the phone - right (2317 images)\n",
    "    - c3 : texting - left (2346 images)\n",
    "    - c4 : talking on the phone - left (2326 images)\n",
    "    - c5 : operating the radio (2312 images)\n",
    "    - c6 : drinking (2325 images)\n",
    "    - c7 : reaching behind (2002 images)\n",
    "    - c8 : hair and makeup (1911 images)\n",
    "    - c9 : talking to a passenger (2129 images)\n",
    "\n",
    "\n",
    "- The testing data provided is totally unlabelled as expected.\n",
    "    \n",
    "\n",
    "- My goal for this notebook is to demonstrate an intuitive understanding of going about solving a computer vision problem.\n",
    "\n",
    "\n",
    "- I will be solving this problem by building on top of the Vgg16 model and I will be employing various proven methods that improve accuracy. I will not be going into the mathematical details of approaches, but rather something that can be thought through intuitively such that the process adds up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Steps :\n",
    "\n",
    "- Creating validation + sample sets\n",
    "- Rearranging image files into respective directories\n",
    "- Finetuning & Training model\n",
    "- Generating Predictions\n",
    "- Validating Predictions\n",
    "- Submitting to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in glob('c?'):\n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    os.mkdir('../sample/valid/'+d)\n",
    "    os.mkdir('../valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separated 1950 out of 22424 images from the training set to the validation set.\n",
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(2000):\n",
    "    os.rename(shuf[i], DATA_DIRECTORY+'/valid/'+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating sample data from training & validation data to run as a test for quick iteration\n",
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1500): copyfile(shuf[i], '../sample/train/'+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/data/statefarm/valid\n"
     ]
    }
   ],
   "source": [
    "%cd ../valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob('c?/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(1000): copyfile(shuf[i], '../sample/valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mdriver_imgs_list.csv.zip\u001b[0m  \u001b[01;34mresults\u001b[0m/  \u001b[01;31msample_submission.csv.zip\u001b[0m  \u001b[01;34mtrain\u001b[0m/\r\n",
      "\u001b[01;31mimgs.zip\u001b[0m                  \u001b[01;34msample\u001b[0m/   \u001b[01;34mtest\u001b[0m/                      \u001b[01;34mvalid\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "\n",
    "from __future__ import print_function, division\n",
    "path = \"data/statefarm/\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "conv_layers, _ = split_at(model, Convolution2D)\n",
    "\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Lambda at 0x7f92f9bcf390>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f9a9b710>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f9a647d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f97f30d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f98003d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f92f9a9b6d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f97b4fd0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f974f310>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f97a3c10>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f978af90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f92f97b4f90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f9734350>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f973f750>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f96ecc90>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f96fe510>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f96f91d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f968fc10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f92f9734310>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f96c2d50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f9655150>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f96545d0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f96877d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f963b350>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f96413d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f92f96c2d10>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f9640790>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f95e2b90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f958ee50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f95a1910>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f92f95afad0>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7f92f9550c90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving batches...\n",
      "Found 20424 images belonging to 10 classes.\n",
      "Retrieving val_batches...\n",
      "Found 2000 images belonging to 10 classes.\n",
      "retrieving test_batches...\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving batches...\")\n",
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "print(\"Retrieving val_batches...\")\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "print(\"retrieving test_batches...\")\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving classes, labels and filenames...\n",
      "Found 20424 images belonging to 10 classes.\n",
      "Found 2000 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving classes, labels and filenames...\")\n",
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing Convolution Output from the Vgg model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Precomputing Convolution Output from the Vgg model...\")\n",
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "# conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Saving pre-computed convolutional data\")\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)\n",
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading pre-computed convolutional data\")\n",
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since the pre-computation of the convolutional network is done, the next step is to create a network that takes the\n",
    "# conv output as its input and predicts the 10 driver classes...\n",
    "\n",
    "# A dropout value\n",
    "p=0.8\n",
    "\n",
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precomp_model takes in the\n",
    "def precomp_model(conv_feat, trn_labels, conv_val_feat, val_labels):\n",
    "    print(\"Getting the final classification layers\")\n",
    "    bn_model = Sequential(gen_bn_layers(p))\n",
    "    bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Fitting the bn_model with a learning rate of 0.001\")\n",
    "    bn_model.fit(da_conv_feat, da_trn_labels, batch_size= batch_size, nb_epoch = 1, validation_data=(conv_val_feat, val_labels))\n",
    "\n",
    "    print(\"Fitting the bn_model with a learning rate of 0.01\")\n",
    "    bn_model.optimizer.lr =0.01\n",
    "    bn_model.fit(da_conv_feat, da_trn_labels, batch_size= batch_size, nb_epoch = 2, validation_data=(conv_val_feat, val_labels))\n",
    "\n",
    "    print(\"Model weights are saved in the models directory...\")\n",
    "    bn_model.save_weights(path+'models/precomp_model.h5')\n",
    "\n",
    "    return bn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Compiling precomp_model...\")\n",
    "precomp_model(conv_feat, trn_labels, conv_val_feat, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing submission...\n",
    "print(\"Preparing a submission file...\")\n",
    "def do_clip(arr, mx):\n",
    "    return np.clip(arr, (1-mx)/9, mx)\n",
    "\n",
    "# precomputed convolutional test output\n",
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')\n",
    "\n",
    "# returns a numpy array of predictions\n",
    "predictions = bn_model.predict(conv_test_feat, batch_size=batch_size*2)\n",
    "\n",
    "subm = do_clip(predictions, 0.93)\n",
    "\n",
    "submission_name = path+'results/toSubmit1.csv'\n",
    "\n",
    "# classes\n",
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "\n",
    "# CSV submission reay file\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "\n",
    "submission.to_csv(submission_name, index=False, encoding='utf-8')\n",
    "print(\"Donezo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
